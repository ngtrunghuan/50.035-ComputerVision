{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. \n",
    "For more details see the [assignments page]() on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_error(out, correct_out):\n",
    "    return np.sum(abs(out - correct_out) / (abs(out) + abs(correct_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \n",
    "    # # We will also make a development set, which is a small subset of\n",
    "    # the training set.\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "\n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "\n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "print 'dev data shape: ', X_dev.shape\n",
    "print 'dev labels shape: ', y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression as classifier\n",
    "\n",
    "$$XW=y$$\n",
    "Where:\n",
    "* $W\\in \\mathbb{R}^{(d+1)\\times C}$\n",
    "* $X\\in \\mathbb{R}^{n\\times (d+1)}$\n",
    "* $y\\in \\mathbb{R}^{n\\times C}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot vectors for label\n",
    "num_class = 10\n",
    "y_train_oh = np.zeros((y_train.shape[0], 10))\n",
    "y_train_oh[np.arange(y_train.shape[0]), y_train] = 1\n",
    "y_val_oh = np.zeros((y_val.shape[0], 10))\n",
    "y_val_oh[np.arange(y_val.shape[0]), y_val] = 1\n",
    "y_test_oh = np.zeros((y_test.shape[0], 10))\n",
    "y_test_oh[np.arange(y_test.shape[0]), y_test] = 1\n",
    "\n",
    "y_dev_oh = np.zeros((y_dev.shape[0], 10))\n",
    "y_dev_oh[np.arange(y_dev.shape[0]), y_dev] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search\n",
    "\n",
    "To evaluate how good $XW$ approximate the label matrix $y$. We can use **Frobenius norm**.\n",
    "* Frobenius norm of $m\\times n$ matrix $A$ is defined as the square root of the sum of the absolute squares of its elements,:\n",
    "$$\\|A\\|_F=\\sqrt{\\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^nA_{ij}^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Frobenius_norm(A):\n",
    "    Fnorm = None\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Implement a function to calculate Frobenius Norm of matrix A.                #\n",
    "    # Hint: It is fine to use 2-nested for-loop. However, you can implement this   #\n",
    "    # function with matrix calculation, which is much faster.                      #\n",
    "    ################################################################################\n",
    "    pass\n",
    "    Fnorm = np.sqrt(np.trace(A.dot(A.T)))\n",
    "    ################################################################################\n",
    "    #                              END OF YOUR CODE                                #\n",
    "    ################################################################################\n",
    "    return Fnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The difference: ', 7.329050276784479e-17)\n"
     ]
    }
   ],
   "source": [
    "# Check the accuracy of your implementation\n",
    "A = np.random.rand(3,2)\n",
    "print('The difference: ', rel_error(Frobenius_norm_naive(A), np.linalg.norm(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in attempt 0 the loss was 32.224197, best 32.224197\n",
      "in attempt 1 the loss was 34.708444, best 32.224197\n",
      "in attempt 2 the loss was 32.788618, best 32.224197\n",
      "in attempt 3 the loss was 32.473939, best 32.224197\n",
      "in attempt 4 the loss was 32.594263, best 32.224197\n",
      "in attempt 5 the loss was 34.191066, best 32.224197\n",
      "in attempt 6 the loss was 32.215763, best 32.215763\n",
      "in attempt 7 the loss was 32.303602, best 32.215763\n",
      "in attempt 8 the loss was 33.307417, best 32.215763\n",
      "in attempt 9 the loss was 31.605834, best 31.605834\n",
      "in attempt 10 the loss was 32.064359, best 31.605834\n",
      "in attempt 11 the loss was 33.127213, best 31.605834\n",
      "in attempt 12 the loss was 35.353552, best 31.605834\n",
      "in attempt 13 the loss was 31.737563, best 31.605834\n",
      "in attempt 14 the loss was 36.842607, best 31.605834\n",
      "in attempt 15 the loss was 33.725396, best 31.605834\n",
      "in attempt 16 the loss was 31.309149, best 31.309149\n",
      "in attempt 17 the loss was 32.374827, best 31.309149\n",
      "in attempt 18 the loss was 33.352039, best 31.309149\n",
      "in attempt 19 the loss was 33.352592, best 31.309149\n",
      "in attempt 20 the loss was 32.776241, best 31.309149\n",
      "in attempt 21 the loss was 35.922740, best 31.309149\n",
      "in attempt 22 the loss was 32.397767, best 31.309149\n",
      "in attempt 23 the loss was 34.797894, best 31.309149\n",
      "in attempt 24 the loss was 35.056880, best 31.309149\n",
      "in attempt 25 the loss was 32.072461, best 31.309149\n",
      "in attempt 26 the loss was 31.729372, best 31.309149\n",
      "in attempt 27 the loss was 33.980780, best 31.309149\n",
      "in attempt 28 the loss was 32.852052, best 31.309149\n",
      "in attempt 29 the loss was 33.174933, best 31.309149\n",
      "in attempt 30 the loss was 34.547286, best 31.309149\n",
      "in attempt 31 the loss was 34.702593, best 31.309149\n",
      "in attempt 32 the loss was 35.305035, best 31.309149\n",
      "in attempt 33 the loss was 34.515115, best 31.309149\n",
      "in attempt 34 the loss was 34.942519, best 31.309149\n",
      "in attempt 35 the loss was 35.580770, best 31.309149\n",
      "in attempt 36 the loss was 32.550545, best 31.309149\n",
      "in attempt 37 the loss was 33.749125, best 31.309149\n",
      "in attempt 38 the loss was 31.536913, best 31.309149\n",
      "in attempt 39 the loss was 32.585115, best 31.309149\n",
      "in attempt 40 the loss was 30.403252, best 30.403252\n",
      "in attempt 41 the loss was 31.715239, best 30.403252\n",
      "in attempt 42 the loss was 32.571900, best 30.403252\n",
      "in attempt 43 the loss was 34.088579, best 30.403252\n",
      "in attempt 44 the loss was 33.046605, best 30.403252\n",
      "in attempt 45 the loss was 36.035078, best 30.403252\n",
      "in attempt 46 the loss was 33.771112, best 30.403252\n",
      "in attempt 47 the loss was 34.259246, best 30.403252\n",
      "in attempt 48 the loss was 33.807841, best 30.403252\n",
      "in attempt 49 the loss was 34.269036, best 30.403252\n",
      "in attempt 50 the loss was 32.715936, best 30.403252\n",
      "in attempt 51 the loss was 31.480383, best 30.403252\n",
      "in attempt 52 the loss was 31.964153, best 30.403252\n",
      "in attempt 53 the loss was 33.402264, best 30.403252\n",
      "in attempt 54 the loss was 35.851256, best 30.403252\n",
      "in attempt 55 the loss was 36.568403, best 30.403252\n",
      "in attempt 56 the loss was 31.513765, best 30.403252\n",
      "in attempt 57 the loss was 37.212323, best 30.403252\n",
      "in attempt 58 the loss was 32.889593, best 30.403252\n",
      "in attempt 59 the loss was 35.816712, best 30.403252\n",
      "in attempt 60 the loss was 34.260975, best 30.403252\n",
      "in attempt 61 the loss was 32.319157, best 30.403252\n",
      "in attempt 62 the loss was 35.359351, best 30.403252\n",
      "in attempt 63 the loss was 31.408159, best 30.403252\n",
      "in attempt 64 the loss was 32.092351, best 30.403252\n",
      "in attempt 65 the loss was 33.166820, best 30.403252\n",
      "in attempt 66 the loss was 34.208223, best 30.403252\n",
      "in attempt 67 the loss was 31.890895, best 30.403252\n",
      "in attempt 68 the loss was 37.897686, best 30.403252\n",
      "in attempt 69 the loss was 34.315550, best 30.403252\n",
      "in attempt 70 the loss was 33.111071, best 30.403252\n",
      "in attempt 71 the loss was 35.247391, best 30.403252\n",
      "in attempt 72 the loss was 34.096658, best 30.403252\n",
      "in attempt 73 the loss was 33.124967, best 30.403252\n",
      "in attempt 74 the loss was 37.795092, best 30.403252\n",
      "in attempt 75 the loss was 33.923407, best 30.403252\n",
      "in attempt 76 the loss was 34.349864, best 30.403252\n",
      "in attempt 77 the loss was 31.194180, best 30.403252\n",
      "in attempt 78 the loss was 33.443666, best 30.403252\n",
      "in attempt 79 the loss was 34.795650, best 30.403252\n",
      "in attempt 80 the loss was 36.680136, best 30.403252\n",
      "in attempt 81 the loss was 31.891491, best 30.403252\n",
      "in attempt 82 the loss was 32.538075, best 30.403252\n",
      "in attempt 83 the loss was 32.837614, best 30.403252\n",
      "in attempt 84 the loss was 33.714587, best 30.403252\n",
      "in attempt 85 the loss was 31.725292, best 30.403252\n",
      "in attempt 86 the loss was 33.177458, best 30.403252\n",
      "in attempt 87 the loss was 34.769338, best 30.403252\n",
      "in attempt 88 the loss was 32.062635, best 30.403252\n",
      "in attempt 89 the loss was 33.571459, best 30.403252\n",
      "in attempt 90 the loss was 34.140058, best 30.403252\n",
      "in attempt 91 the loss was 33.203769, best 30.403252\n",
      "in attempt 92 the loss was 32.292792, best 30.403252\n",
      "in attempt 93 the loss was 33.251721, best 30.403252\n",
      "in attempt 94 the loss was 33.062088, best 30.403252\n",
      "in attempt 95 the loss was 31.154841, best 30.403252\n",
      "in attempt 96 the loss was 36.399335, best 30.403252\n",
      "in attempt 97 the loss was 35.403816, best 30.403252\n",
      "in attempt 98 the loss was 33.636284, best 30.403252\n",
      "in attempt 99 the loss was 31.945212, best 30.403252\n"
     ]
    }
   ],
   "source": [
    "bestloss = float('inf')\n",
    "for num in xrange(100):\n",
    "    W = np.random.randn(3073, 10) * 0.0001\n",
    "    loss = np.linalg.norm(X_dev.dot(W) - y_dev_oh)\n",
    "    if (loss < bestloss):\n",
    "        bestloss = loss\n",
    "        bestW = W\n",
    "    print('in attempt %d the loss was %f, best %f' % (num, loss, bestloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set:  10.4\n",
      "Accuracy on test set:  9.2\n"
     ]
    }
   ],
   "source": [
    "# How bestW perform:\n",
    "print 'Accuracy on train set: ', np.sum(np.argmin(np.abs(1 - X_dev.dot(W)), axis=1) == y_dev).astype(np.float32)/y_dev.shape[0]*100\n",
    "print 'Accuracy on test set: ', np.sum(np.argmin(np.abs(1 - X_test.dot(W)), axis=1) == y_test).astype(np.float32)/y_test.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed-form solution\n",
    "The closed-form solution is: \n",
    "$$W=(X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Implement the closed-form solution of the weight W.                          #\n",
    "################################################################################\n",
    "pass\n",
    "W = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train_oh)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy:  51.1632653061\n",
      "Test set accuracy:  36.2\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy:\n",
    "print 'Train set accuracy: ', np.sum(np.argmin(np.abs(1 - X_train.dot(W)), axis=1) == y_train).astype(np.float32)/y_train.shape[0]*100\n",
    "print 'Test set accuracy: ', np.sum(np.argmin(np.abs(1 - X_test.dot(W)), axis=1) == y_test).astype(np.float32)/y_test.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "The predicted probability for the $j$-th class given a sample vector $x$ and a weight $W$ is:\n",
    "\n",
    "$$\\displaystyle{P(y=j\\mid x)=\\frac{e^{-xw_j}}{\\sum\\limits_{c=1}^{C}e^{-xw_c}}}$$ \n",
    "\n",
    "Your code for this section will all be written inside **classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.295517\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestloss = float('inf')\n",
    "for num in xrange(1000):\n",
    "    W = np.random.randn(3073, 10) * 0.0001\n",
    "    loss, _ = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "    if (loss < bestloss):\n",
    "        bestloss = loss\n",
    "        bestW = W\n",
    "    print('in attempt %d the loss was %f, best %f' % (num, loss, bestloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How bestW perform on trainset\n",
    "scores = X_train.dot(bestW)\n",
    "y_pred = np.argmax(scores, axis=1)\n",
    "print('Accuracy on train set %f' % np.mean(y_pred == y_train))\n",
    "\n",
    "# evaluate performance of test set\n",
    "scores = X_test.dot(bestW)\n",
    "y_pred = np.argmax(scores, axis=1)\n",
    "print('Accuracy on test set %f' % np.mean(y_pred == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.962821 analytic: 0.962821, relative error: 1.214148e-07\n",
      "numerical: -0.774558 analytic: -0.774558, relative error: 1.800527e-08\n",
      "numerical: 1.299720 analytic: 1.299720, relative error: 1.946793e-08\n",
      "numerical: 3.192995 analytic: 3.192995, relative error: 8.821473e-09\n",
      "numerical: 1.354179 analytic: 1.354179, relative error: 1.502823e-08\n",
      "numerical: -4.297725 analytic: -4.297725, relative error: 1.945297e-08\n",
      "numerical: 2.805974 analytic: 2.805974, relative error: 7.558344e-09\n",
      "numerical: 0.119529 analytic: 0.119529, relative error: 1.407189e-07\n",
      "numerical: 1.875345 analytic: 1.875345, relative error: 3.674796e-09\n",
      "numerical: -0.741492 analytic: -0.741492, relative error: 3.922840e-08\n",
      "numerical: 0.997245 analytic: 0.997245, relative error: 2.360949e-08\n",
      "numerical: -3.340166 analytic: -3.340166, relative error: 3.534046e-09\n",
      "numerical: -1.905434 analytic: -1.905434, relative error: 1.438271e-08\n",
      "numerical: -6.017358 analytic: -6.017358, relative error: 2.013794e-09\n",
      "numerical: 2.476932 analytic: 2.476932, relative error: 2.139362e-08\n",
      "numerical: 0.386960 analytic: 0.386960, relative error: 1.436681e-07\n",
      "numerical: 1.462943 analytic: 1.462943, relative error: 1.090339e-08\n",
      "numerical: 0.657187 analytic: 0.657187, relative error: 3.967986e-08\n",
      "numerical: 1.109387 analytic: 1.109387, relative error: 3.901009e-08\n",
      "numerical: 0.101213 analytic: 0.101213, relative error: 5.166143e-07\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.346165e+00 computed in 0.126850s\n",
      "vectorized loss: 2.346165e+00 computed in 0.017583s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)\n",
    "\n",
    "from classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved during cross-validation: -1.000000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [5e4, 1e8]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
